{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e65e195e-9dbe-4c26-b47d-a5f51bb048d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.models\n",
    "import pickle\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51e9ea78-f4bb-4ae9-b9b0-03966c927584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets.utils import download_url\n",
    "import os\n",
    "import tarfile\n",
    "import hashlib\n",
    "\n",
    "# https://github.com/fastai/imagenette\n",
    "# dataset_url = 'https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz'\n",
    "# dataset_filename = dataset_url.split('/')[-1]\n",
    "# dataset_foldername = dataset_filename.split('.')[0]\n",
    "# data_path = './data'\n",
    "# dataset_filepath = os.path.join(data_path,dataset_filename)\n",
    "# dataset_folderpath = os.path.join(data_path,dataset_foldername)\n",
    "\n",
    "# os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "# download = False\n",
    "# if not os.path.exists(dataset_filepath):\n",
    "#     download = True\n",
    "# # else:\n",
    "# #     md5_hash = hashlib.md5()\n",
    "\n",
    "\n",
    "# #     file = open(dataset_filepath, \"rb\")\n",
    "\n",
    "# #     content = file.read()\n",
    "\n",
    "# #     md5_hash.update(content)\n",
    "\n",
    "\n",
    "# #     digest = md5_hash.hexdigest()\n",
    "# #     if digest != 'fe2fc210e6bb7c5664d602c3cd71e612':\n",
    "# #         download = True\n",
    "# if download:\n",
    "#     download_url(dataset_url, data_path)\n",
    "\n",
    "#     with tarfile.open(dataset_filepath, 'r:gz') as tar:\n",
    "#         tar.extractall(path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f767aa5b-50e7-407e-9302-258cc012e0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 12, 19, 13, 19, 1, 10, 12, 16, 11]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler = torch.utils.data.WeightedRandomSampler(torch.ones(20), 10, replacement=True)\n",
    "list(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24ec3b27-f4f7-452c-9591-ed0b0eaa0075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 0, 13, 10, 18, 8, 16, 4, 2, 6]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba4ff40-e31e-455e-8eb5-9436bb896a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoCropsTransform:\n",
    "    \"\"\"Take two random crops of one image as the query and key.\"\"\"\n",
    "\n",
    "    def __init__(self, base_transform):\n",
    "        self.base_transform = base_transform\n",
    "\n",
    "    def __call__(self, x):\n",
    "        q = self.base_transform(x)\n",
    "        k = self.base_transform(x)\n",
    "        return [q, k]\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '(\\n\\t'\n",
    "        format_string += self.base_transform.__repr__().replace('\\n', '\\n\\t')\n",
    "        format_string += '\\n)'\n",
    "        return format_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "610a29b1-3fbf-4426-a6bd-7473d7533d20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import ImageFilter\n",
    "import random\n",
    "\n",
    "class GaussianBlur(object):\n",
    "    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n",
    "\n",
    "    def __init__(self, sigma=[.1, 2.]):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __call__(self, x):\n",
    "        sigma = random.uniform(self.sigma[0], self.sigma[1])\n",
    "        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f84ba021-0047-4d87-b354-f0afd53217c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size  = 224\n",
    "# ks = (int(0.1 * size) // 2) * 2 + 1\n",
    "# __imagenet_stats = {'mean': [0.485, 0.456, 0.406],\n",
    "#                     'std': [0.229, 0.224, 0.225]}\n",
    "\n",
    "#TODO: chose transforms\n",
    "\n",
    "# train_transform = TwoCropsTransform(transforms.Compose([transforms.RandomResizedCrop(scale=(0.2, 1), size=size),\n",
    "#                                         transforms.RandomApply([transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)], p=0.8),                  \n",
    "#                                         transforms.RandomGrayscale(p=0.2),\n",
    "#                                         transforms.RandomApply([GaussianBlur()], p=0.1),\n",
    "#                                         transforms.RandomHorizontalFlip(),\n",
    "#                                         transforms.ToTensor(),\n",
    "#                                         transforms.Normalize(**__imagenet_stats)]))\n",
    "\n",
    "# dataset_train = torchvision.datasets.ImageFolder(os.path.join(dataset_folderpath,'train'), train_transform)\n",
    "#valid_ds = ImageFolder('./data/imagenette-160/val', valid_tfms)\n",
    "\n",
    "# batch_size = 64\n",
    "# train_dataloader = torch.utils.data.DataLoader(\n",
    "#         dataset_train,\n",
    "#         batch_size=batch_size,\n",
    "#         num_workers=8,\n",
    "#         drop_last=True,\n",
    "#         shuffle=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb7dbe77-80d0-4475-aecf-0dbc3834d499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "def contrastive_loss(z1, z2, tau=0.2):\n",
    "    N = z1.shape[0]\n",
    "    logits = torch.mm(z1, z2.t())  # [N, N] pairs\n",
    "    labels = torch.arange(N).cuda()  # positives are in diagonal\n",
    "    loss = F.cross_entropy(logits / tau, labels)\n",
    "    return 2 * tau * loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c2f12b8-8dd5-44c9-874f-70e5afc9af73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MoCoV2(nn.Module):\n",
    "    \n",
    "#     def __init__(self, Q_size=6400, C=128):\n",
    "#         super(MoCoV2, self).__init__()\n",
    "#         self.C = C\n",
    "#         self.Q_size = Q_size\n",
    "#         self.queue_head = 0\n",
    "#         self.queue = torch.randn(C, Q_size, requires_grad=False).cuda()\n",
    "#         self.queue = nn.functional.normalize(self.queue, dim=0)\n",
    "#         self.f_k = torchvision.models.resnet50(num_classes=C)\n",
    "#         self.f_q = torchvision.models.resnet50(num_classes=C)\n",
    "#         dim_mlp = self.f_q.fc.weight.shape[1]\n",
    "#         self.f_q.fc = nn.Sequential(nn.Linear(dim_mlp, dim_mlp), nn.ReLU(), self.f_q.fc)\n",
    "#         self.f_k.fc = nn.Sequential(nn.Linear(dim_mlp, dim_mlp), nn.ReLU(), self.f_k.fc)\n",
    "        \n",
    "#         for param_q, param_k in zip(self.f_q.parameters(), self.f_k.parameters()):\n",
    "#             param_k.data.copy_(param_q.data)\n",
    "#             param_k.requires_grad = False     \n",
    "\n",
    "#     def forward(self, x1, x2):\n",
    "#         N = x1.shape[0]\n",
    "#         query = self.f_q(x1)\n",
    "#         query = nn.functional.normalize(query, dim=1)\n",
    "#         with torch.no_grad():\n",
    "#             k = self.f_k(x2)\n",
    "#             k = nn.functional.normalize(k, dim=1)\n",
    "        \n",
    "#         l_pos = torch.bmm(query.view(N, 1, self.C), k.view(N, self.C, 1)).view(N,1)\n",
    "#         l_neg = torch.mm(query.view(N, self.C), self.queue.view(self.C, self.Q_size).clone().detach())\n",
    "        \n",
    "#         logits = torch.cat([l_pos, l_neg], dim=1)\n",
    "        \n",
    "#         #enqueu dequeue\n",
    "#         self.queue[:, self.queue_head: self.queue_head+ N] = k.clone().t()\n",
    "#         self.queue_head = (self.queue_head + N) % self.Q_size\n",
    "#         return logits        \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f0cc1e5-e6cc-4d36-936d-cfd094ac51d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MoCoV2()\n",
    "\n",
    "\n",
    "# model.to('cuda')\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c06dd47-8def-4735-b2bd-5006eec5034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''Load state dictionary from stopping point'''\n",
    "# import os\n",
    "# try:\n",
    "#     files = [f for f in os.listdir('.') if os.path.isfile(f)]\n",
    "#     epochs_lst = [int(f.split('_')[1]) for f in files if \"snapshot\" in f]\n",
    "#     last_epoch = max(epochs_lst)\n",
    "#     model.load_state_dict(torch.load(f\"snapshot_{last_epoch}\"))\n",
    "# except:\n",
    "#     last_epoch = 0\n",
    "    \n",
    "# print(last_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a61be56-fd1a-40a1-821e-ecae94cd2a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.notebook  import tqdm\n",
    "# import pickle\n",
    "# epochs = 1000\n",
    "# print_step = 10\n",
    "# tau = 0.2\n",
    "# m = 0.999\n",
    "# from matplotlib import pyplot as plt\n",
    "# errors = []\n",
    "\n",
    "# return # we have already trained the Feature extractor!\n",
    "\n",
    "# #lr = 5e-2 * 0.9 ** last_epoch\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=5e-2, momentum=0.9)\n",
    "\n",
    "# lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, eta_min=0)\n",
    "# for i in range(last_epoch):\n",
    "#     lr_sched.step()\n",
    "# optimizer.zero_grad()\n",
    "\n",
    "\n",
    "# if not os.path.exists(dataset_filepath):\n",
    "#     loss_log = []\n",
    "#     epoch_loss = []\n",
    "# else:\n",
    "#     loss_log, epoch_loss = pickle.load(open(\"loss_logs.pkl\", \"rb\"))\n",
    "\n",
    "# def train():\n",
    "#     for epoch in range(last_epoch + 1, epochs + 1):\n",
    "#         total_loss = 0\n",
    "#         for batch_idx, ((x1,x2),_) in enumerate(tqdm(train_dataloader)):\n",
    "#             x1, x2 = x1.cuda(), x2.cuda()\n",
    "#             logits = model(x1, x2)\n",
    "#             labels = torch.zeros(x1.shape[0], dtype=torch.long).cuda()\n",
    "#             loss = nn.functional.cross_entropy(logits / tau, labels)\n",
    "#             if loss.isnan().item():\n",
    "#                 print(\"nan loss\")\n",
    "#                 global errors\n",
    "#                 errors = [x1, x2]\n",
    "#                 print(logits)\n",
    "#                 return\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             with torch.no_grad():\n",
    "#                 for param_q, param_k in zip(model.f_q.parameters(), model.f_k.parameters()):\n",
    "#                     param_k.data = m * param_k.data + (1-m) * param_q.data\n",
    "#             total_loss += loss.item()\n",
    "#             loss_log.append(loss.item())\n",
    "#             if batch_idx % print_step == 0:\n",
    "#                 tqdm.write('Loss {:.4f}'.format(loss.item()))\n",
    "#         lr_sched.step()\n",
    "#         print(f\"update lr to: {optimizer.param_groups[0]['lr']}\")\n",
    "#         ep_loss = total_loss/len(train_dataloader)\n",
    "#         print('Epoch {}: mean loss={:.4f}'.format(epoch, ep_loss))\n",
    "#         epoch_loss.append(ep_loss)\n",
    "#         if epoch % 10 == 0:\n",
    "#             torch.save(model.state_dict(), f\"snapshot_{epoch}\")\n",
    "#             with open(\"loss_logs.pkl\", \"wb\") as f:\n",
    "#                 pickle.dump([loss_log, epoch_loss], f)\n",
    "        \n",
    "             \n",
    "# # train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5294f62-9c29-415e-b7e0-7da7d47c3af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''Loading the Contrastive Loss logs'''\n",
    "# with open(\"loss_logs.pkl\",'rb') as f:\n",
    "#     loss = pickle.load(f)\n",
    "#     f.close()\n",
    "    \n",
    "# from matplotlib import pyplot as plt\n",
    "# plt.plot(loss[1])\n",
    "# # plt.xscale('log')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Contrastive Loss')\n",
    "# plt.figure()\n",
    "\n",
    "# snaps_hist = []\n",
    "# for i in range(100):\n",
    "#     snaps_hist.append(loss[1][i*10])\n",
    "# print(f\"best performing snapshot :{torch.argmin(torch.tensor(snaps_hist))*10} with loss {min(snaps_hist)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7127213c-8cb7-4484-ae54-765a78f861af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('number of parameters: ', sum(param.numel() for param in model.parameters()))\n",
    "# print('Num of trainable parameters : ', sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90808d2b-2079-4ec1-a5a6-597fd8c1be03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading noise file cifar-10-batches-py/cifar-10-batches-py//0.9_sym.json\n"
     ]
    }
   ],
   "source": [
    "import dataloader_cifar as dataloader\n",
    "\n",
    "# Dataset loader\n",
    "\n",
    "def imagenette_dataset():\n",
    "    size  = 224\n",
    "    ks = (int(0.1 * size) // 2) * 2 + 1\n",
    "    __imagenet_stats = {'mean': [0.485, 0.456, 0.406],\n",
    "                        'std': [0.229, 0.224, 0.225]}\n",
    "\n",
    "    #TODO: chose transforms\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "                                            transforms.RandomResizedCrop(224),\n",
    "                                            transforms.RandomHorizontalFlip(),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize(**__imagenet_stats)])\n",
    "\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "                                            transforms.Resize(256),\n",
    "                                            transforms.CenterCrop(224),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize(**__imagenet_stats)])\n",
    "\n",
    "    dataset_train = torchvision.datasets.ImageFolder(os.path.join(dataset_folderpath,'train'), train_transform)\n",
    "    valid_ds = torchvision.datasets.ImageFolder(os.path.join(dataset_folderpath,'val'), val_transform)\n",
    "\n",
    "    batch_size = 64\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "            dataset_train,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=2,\n",
    "            drop_last=True,\n",
    "            shuffle=True,\n",
    "    )\n",
    "\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "            valid_ds,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=2,\n",
    "            drop_last=True,\n",
    "            shuffle=False,\n",
    "    )\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "loader = None\n",
    "\n",
    "def cifar10_dataset(noise_ratio=0.9):\n",
    "    stats_file = open('stats_file.txt', 'w')\n",
    "    noise_mode='sym'\n",
    "    batch_size=32\n",
    "    data_path='cifar-10-batches-py/cifar-10-batches-py/'\n",
    "    global loader\n",
    "    loader = dataloader.cifar_dataloader('cifar10',r=noise_ratio,noise_mode=noise_mode,batch_size=batch_size,num_workers=5,\\\n",
    "        root_dir=data_path,log=stats_file,noise_file='%s/%.1f_%s.json'%(data_path,noise_ratio,noise_mode))\n",
    "\n",
    "    # batch shape: images, noise_label, index\n",
    "    train_dataloader = loader.run('warmup')\n",
    "\n",
    "    # first_batch = next(iter(train_dataloader))\n",
    "    # for i in first_batch:\n",
    "    #     print(i.shape)\n",
    "    # print(first_batch)\n",
    "\n",
    "    val_dataloader = loader.run('test')\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "\n",
    "train_dataloader, val_dataloader = cifar10_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc4ad43e-9b8f-40f2-932f-99c5a05d7e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate(model, val_dataloader, criterion):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        top1_sum = 0\n",
    "        top5_sum = 0\n",
    "        c = 0\n",
    "        for x,y in tqdm(val_dataloader):\n",
    "            x = x.cuda()\n",
    "            # x = torch.randn([64, 128]).cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            y_hat = model(x)\n",
    "\n",
    "            acc1, acc5 = accuracy(y_hat, y, (1,5))\n",
    "            top1_sum += y.shape[0] * acc1\n",
    "            top5_sum += y.shape[0] * acc5\n",
    "            c += y.shape[0]\n",
    "        \n",
    "        print(f\"top-1: {top1_sum / c}, top-5: {top5_sum / c}\")\n",
    "        return top1_sum.item() / c, top5_sum / c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c5d5394-54ea-454e-8de2-0a60cbc55f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].flatten().float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62f34daf-69ef-4982-901b-85349f498029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LC(nn.Module):\n",
    "\n",
    "    def __init__(self, base_encoder, num_classes=10, learn_encoder=False):\n",
    "        super(LC, self).__init__()\n",
    "        self.encoder = base_encoder\n",
    "        if base_encoder is None:\n",
    "            raise Exception('base encoder is None')\n",
    "        for p in self.encoder.parameters():\n",
    "            p.requires_grad = learn_encoder\n",
    "\n",
    "        self.mlp = nn.Linear(128,num_classes)\n",
    "        self.sm = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sm(self.mlp(self.encoder(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f1b647b-92ce-4b47-bb69-19f3ee9ab031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "snapshot_name = \"simclr_chkpnt/ckpt_cifar10_resnet50.pth\"\n",
    "\n",
    "\n",
    "def fix_state_dict(state_dict):\n",
    "    # create new OrderedDict that does not contain `module.`\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[len('encoder.module.'):] # remove `module.`\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7b1aa29-9bbe-41f7-b947-863937f6d3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet import SupCEResNet\n",
    "net = \"resnet50\"\n",
    "chekpoint = torch.load(snapshot_name)\n",
    "sd = {}\n",
    "for ke in chekpoint['model']:\n",
    "    nk = ke.replace('module.', '')\n",
    "    sd[nk] = chekpoint['model'][ke]\n",
    "model = SupCEResNet(net, num_classes=10)\n",
    "for p in model.encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "model.load_state_dict(sd, strict=False)\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03452998-2b20-42a0-b635-a82017cd876a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learnable paramters: 20490\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e924bac97fa4d0f897bfefe42b1ce2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "top-1: tensor([83.5200], device='cuda:0'), top-5: tensor([98.4300], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(83.52, tensor([98.4300], device='cuda:0'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.notebook  import tqdm\n",
    "import copy\n",
    "from resnet import resnet50\n",
    "# Stage-2\n",
    "dw_epochs = 20\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "print_step = 100\n",
    "out_dir=\"cifar10/clf_learning\"\n",
    "mlp_clf = model\n",
    "# validate(mlp_clf, val_dataloader, criterion)\n",
    "print(f\"learnable paramters: {sum(torch.numel(p) for p in mlp_clf.parameters() if p.requires_grad)}\")\n",
    "\n",
    "# optimizer\n",
    "lc_opt = torch.optim.SGD(mlp_clf.parameters(), lr=3e-4 , momentum=0.9)\n",
    "lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(lc_opt, dw_epochs, 0)\n",
    "lc_opt.zero_grad()\n",
    "\n",
    "\n",
    "lin_loss_log = []\n",
    "lin_epoch_loss = []\n",
    "acc1_hist = []\n",
    "acc5_hist = []\n",
    "\n",
    "def mlp_train():\n",
    "    model.eval()\n",
    "    best_model_wts = copy.deepcopy(mlp_clf.state_dict())\n",
    "    best_val = 0\n",
    "    for epoch in range(1, dw_epochs + 1):\n",
    "        total_loss = 0\n",
    "        # images, noise_label, index\n",
    "        for batch_idx, (x, y, index) in enumerate(tqdm(train_dataloader)):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            y_hat = mlp_clf(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            \n",
    "            lc_opt.zero_grad()\n",
    "            loss.backward()\n",
    "            lc_opt.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            lin_loss_log.append(loss.item())\n",
    "            if batch_idx % print_step == 0:\n",
    "                tqdm.write('Loss {:.4f}'.format(loss.item()))\n",
    "\n",
    "        current_epoch_loss = total_loss/len(train_dataloader)\n",
    "        lin_epoch_loss.append(current_epoch_loss)\n",
    "        print('Epoch {}: mean loss={:.4f}'.format(epoch, current_epoch_loss))\n",
    "        lr_sched.step()\n",
    "        print(f\"update lr to: {lc_opt.param_groups[0]['lr']}\")\n",
    "        val,val5 = validate(mlp_clf, val_dataloader, criterion)\n",
    "        acc1_hist.append(val)\n",
    "        acc5_hist.append(val5)\n",
    "        print(f'Accuracy on validation set: {val}' )\n",
    "        if val > best_val:\n",
    "            best_val = val\n",
    "            best_model_wts = copy.deepcopy(mlp_clf.state_dict())\n",
    "\n",
    "        mlp_clf.train()\n",
    "        print(f\"learnable paramters: {sum(torch.numel(p) for p in mlp_clf.parameters() if p.requires_grad)}\")\n",
    "        if epoch % 10 == 0:\n",
    "            with open(f\"{out_dir}/classifier_loss_logs.pkl\", \"wb\") as f:\n",
    "                pickle.dump([lin_loss_log, lin_epoch_loss], f)\n",
    "    return best_model_wts, best_val\n",
    "\n",
    "# wts, best_val = mlp_train()\n",
    "# torch.save(wts, f\"{out_dir}/final_model_wts.bin\")\n",
    "model.load_state_dict(torch.load(f\"{out_dir}/final_model_wts.bin\"))\n",
    "# validate(mlp_clf, val_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6191f77e-230b-41d2-ab50-dda97125e1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Loading the Classifier Loss logs'''\n",
    "with open(f\"{out_dir}/classifier_loss_logs.pkl\",'rb') as f:\n",
    "    loss = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(loss[1])\n",
    "# plt.xscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classifier Loss')\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "''' plotting the Top-1 & Top-5 accuracy'''\n",
    "plt.plot(acc1_hist)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Top-1 Accuracy')\n",
    "plt.figure()\n",
    "hist5 = [a.item() for a in acc5_hist]\n",
    "plt.plot(hist5)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Top-5 Accuracy')\n",
    "plt.figure()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57ed38c4-7c50-492a-9c87-7bfc65d29613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading noise file cifar-10-batches-py/cifar-10-batches-py//0.9_sym.json\n",
      "loading noise file cifar-10-batches-py/cifar-10-batches-py//0.9_sym.json\n",
      "labeled data has a size of 7888\n",
      "using CBS\n",
      "loading noise file cifar-10-batches-py/cifar-10-batches-py//0.9_sym.json\n",
      "unlabeled data has a size of 42112\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "import dataloader_cifar as dataloader\n",
    "\n",
    "def get_train(model, dataloader, estimate_p_right=True, p_right_threshold=0.5, p_clean_threshold=0.5, CBS=True):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(len(dataloader.dataset))\n",
    "    confidence = torch.zeros(len(dataloader.dataset))\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets, index) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda() \n",
    "            outputs = model(inputs)\n",
    "            loss = nn.CrossEntropyLoss(reduction='none')(outputs, targets)\n",
    "            probs = torch.max(torch.softmax(outputs, dim=-1), dim=-1).values\n",
    "            for b in range(inputs.size(0)):\n",
    "                losses[index[b]]=loss[b]\n",
    "                confidence[index[b]] = probs[b]\n",
    "    losses = (losses-losses.min())/(losses.max()-losses.min())\n",
    "\n",
    "    input_loss = losses.reshape(-1, 1)\n",
    "    confidence = confidence.reshape(-1, 1)\n",
    "    # fit a two-component GMM to the loss\n",
    "    gmm1 = GaussianMixture(n_components=2,max_iter=10,tol=1e-2,reg_covar=5e-4)\n",
    "    gmm1.fit(input_loss)\n",
    "    prob = gmm1.predict_proba(input_loss)\n",
    "    p_clean = prob[:,gmm1.means_.argmin()]\n",
    "    clean = (p_clean > p_clean_threshold)\n",
    "    if estimate_p_right:\n",
    "        gmm2 = GaussianMixture(n_components=2,max_iter=10,tol=1e-2,reg_covar=5e-4)\n",
    "        gmm2.fit(confidence)\n",
    "        prob = gmm2.predict_proba(confidence)\n",
    "        p_right = prob[:, gmm2.means_.argmax()]\n",
    "        right = (p_right > p_right_threshold)\n",
    "        clean = clean | right\n",
    "    return loader.run('train', clean, p_clean, CBS)\n",
    "\n",
    "labeled_trainloader, unlabeled_trainloader = get_train(model, loader.run('eval_train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f19002db-66d8-49a8-b08f-06f3962fdd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = torch.zeros(10)\n",
    "# print(counter)\n",
    "# for i in labeled_trainloader.dataset:\n",
    "#     counter[i[2]] += 1\n",
    "# print(counter)\n",
    "# counter / sum(counter)\n",
    "i = 0\n",
    "hist = torch.zeros(10)\n",
    "for b in labeled_trainloader:\n",
    "    for j in b[2]:\n",
    "        hist[j] += 1\n",
    "    i+= 1\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b247965f-b4a1-48f6-b8a0-3b4684e64b68",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-06e359bb4625>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "047c4f5a-907e-4a79-a1cb-3b9a9450900a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([771., 730., 807., 840., 765., 835., 808., 791., 769., 772.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f0118df-8189-4e0e-a81f-f211af6010aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-bb177b70d948>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msorteddel_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_out' is not defined"
     ]
    }
   ],
   "source": [
    "sorteddel_out = model(torch.rand(5, 3, 32, 32).cuda())\n",
    "print(torch.softmax(model_out, dim=-1))\n",
    "torch.max(torch.softmax(model_out, dim=-1), dim=-1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f783dc85-e910-4a1d-ad42-d32fc7c0393f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806cc54e-8bba-404a-9cae-90e37f9a81d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6da49e-bd32-4a2a-987c-f4e05ef745a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d4d00-886b-4167-b1f9-df4f74cf8fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57765bc-308a-47cb-8142-eac050dfe558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43b6d0-0686-40b4-91ca-304f9c34538b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57047885-821d-47f7-9e0e-6c110f59f597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af4205a-63a3-4bca-8881-79101d70fd52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c6fb7-bb4b-44be-9ef6-7115e1ff2e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9c5220-abf6-48cd-8770-aa101c5e80a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080384af-5f30-470b-b3b0-c03ca62d3a03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac4925-75dd-49fd-9b9e-4555193e2c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b67aa42f-20cd-466a-a3b6-3e6a50c92fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = next(iter(labeled_trainloader))[::2]\n",
    "U = next(iter(unlabeled_trainloader))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60214364-6afa-4bc9-8f3d-6fc4ff98ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out1 = model(U.cuda())\n",
    "model_out1 = torch.softmax(model_out1, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7fc7b26b-5edc-4005-8a05-053feeb507f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out1.shape\n",
    "L1 = torch.zeros_like(model_out1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eff92713-51ed-41f3-beab-557643a18c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 3, 1, 2, 1, 6, 4, 6, 5, 7, 5, 9, 1, 4, 5, 8, 1, 4, 3, 5, 5, 4, 7, 9,\n",
      "        0, 7, 0, 4, 0, 8, 9, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.ones(32,10)\n",
    "print(L[1])\n",
    "torch.zeros(32,10).scatter_(1,L[1].unsqueeze(-1),src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412e6d75-2d6b-490d-a9c5-bc160e868baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "L[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "27b31de6-75b2-4b75-a2d8-9cabb71de76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import ReLU\n",
    "from torch.nn.functional import normalize\n",
    "Re = ReLU()\n",
    "src = torch.ones(32,10)\n",
    "def graph_structured_regularization(model, L, U, lambda_uu, lambda_lu, tau): #U-unlabled class, L-labled, \n",
    "    Z_l = model.encoder(L[0].cuda())\n",
    "    Z_u = model.encoder(U.cuda())\n",
    "    Z_l = normalize(Z_l,dim=1)\n",
    "    Z_u = normalize(Z_u,dim=1)\n",
    "    A_lu = Re(torch.matmul(Z_l, Z_u.t())-tau)\n",
    "    A_uu = Re(torch.matmul(Z_u, Z_u.t())-tau)\n",
    "    y_hat = torch.zeros(32,10).scatter_(1,L[1].unsqueeze(-1),src).cuda()\n",
    "    p = model(U.cuda())\n",
    "    p = torch.softmax(p, dim=-1)\n",
    "    one_vec = torch.ones(32).t().unsqueeze(dim=-1).cuda()\n",
    "    P_mat = kronecker_product(one_vec,p).reshape(32,32,10)\n",
    "    Y_mat = kronecker_product(one_vec,y_hat).reshape(32,32,10)\n",
    "    Y_mat = torch.transpose(Y_mat,0,1)\n",
    "    norm_lu = torch.norm(P_mat - Y_mat, dim = -1)\n",
    "    norm_uu = torch.norm(P_mat - torch.transpose(P_mat,0,1), dim = -1)\n",
    "    #print(norm_lu.shape,norm_uu.shape)\n",
    "    return (lambda_lu*A_lu*norm_lu).sum() + (lambda_uu*A_uu*norm_uu).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c0992f89-e8b9-4325-8be1-cd23f346dc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kronecker_product(t1, t2):\n",
    "    \"\"\"\n",
    "    Computes the Kronecker product between two tensors.\n",
    "    See https://en.wikipedia.org/wiki/Kronecker_product\n",
    "    \"\"\"\n",
    "    t1_height, t1_width = t1.shape\n",
    "    t2_height, t2_width = t2.shape\n",
    "    out_height = t1_height * t2_height\n",
    "    out_width = t1_width * t2_width\n",
    "\n",
    "    tiled_t2 = t2.repeat(t1_height, t1_width)\n",
    "    expanded_t1 = (\n",
    "        t1.unsqueeze(2)\n",
    "          .unsqueeze(3)\n",
    "          .repeat(1, t2_height, t2_width, 1)\n",
    "          .view(out_height, out_width)\n",
    "    )\n",
    "\n",
    "    return expanded_t1 * tiled_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "c4772681-b3ce-462f-bf67-de2fd31e4c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5],\n",
       "        [6, 7]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = torch.tensor(range(8)).reshape(4,2)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "1a6fdce1-4b42-4f97-897e-09a6f3ed9088",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = kronecker_product(torch.ones(4).t().unsqueeze(dim=-1),p.squeeze(dim=1)).reshape(4,4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "4d321a5e-b81a-44ac-93b1-9ddf7e67ebd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1.],\n",
       "         [2., 3.],\n",
       "         [4., 5.],\n",
       "         [6., 7.]],\n",
       "\n",
       "        [[0., 1.],\n",
       "         [2., 3.],\n",
       "         [4., 5.],\n",
       "         [6., 7.]],\n",
       "\n",
       "        [[0., 1.],\n",
       "         [2., 3.],\n",
       "         [4., 5.],\n",
       "         [6., 7.]],\n",
       "\n",
       "        [[0., 1.],\n",
       "         [2., 3.],\n",
       "         [4., 5.],\n",
       "         [6., 7.]]])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ce83cf42-ada7-4185-a80c-b9ae89c0a425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 10])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "f9eac2ff-1a02-4a10-9a64-969e375b8604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(10.)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(10)\n",
    "b = torch.ones(10)\n",
    "c = a * b\n",
    "print(c.shape)\n",
    "c.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d664dbf-d4e3-430e-b0af-826435deeaee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
