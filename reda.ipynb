{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65e195e-9dbe-4c26-b47d-a5f51bb048d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.models\n",
    "import pickle\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1351c986-49b2-4bcd-90ca-c7af06cef220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\n",
    "       Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90808d2b-2079-4ec1-a5a6-597fd8c1be03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataloader_cifar as dataloader\n",
    "\n",
    "# Dataset loader\n",
    "\n",
    "def imagenette_dataset():\n",
    "    size  = 224\n",
    "    ks = (int(0.1 * size) // 2) * 2 + 1\n",
    "    __imagenet_stats = {'mean': [0.485, 0.456, 0.406],\n",
    "                        'std': [0.229, 0.224, 0.225]}\n",
    "\n",
    "    #TODO: chose transforms\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "                                            transforms.RandomResizedCrop(224),\n",
    "                                            transforms.RandomHorizontalFlip(),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize(**__imagenet_stats)])\n",
    "\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "                                            transforms.Resize(256),\n",
    "                                            transforms.CenterCrop(224),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize(**__imagenet_stats)])\n",
    "\n",
    "    dataset_train = torchvision.datasets.ImageFolder(os.path.join(dataset_folderpath,'train'), train_transform)\n",
    "    valid_ds = torchvision.datasets.ImageFolder(os.path.join(dataset_folderpath,'val'), val_transform)\n",
    "\n",
    "    batch_size = 64\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "            dataset_train,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=2,\n",
    "            drop_last=True,\n",
    "            shuffle=True,\n",
    "    )\n",
    "\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "            valid_ds,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=2,\n",
    "            drop_last=True,\n",
    "            shuffle=False,\n",
    "    )\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "loader = None\n",
    "\n",
    "def cifar10_dataset(noise_ratio=0.9):\n",
    "    noise_mode='sym'\n",
    "    batch_size=32\n",
    "    data_path='cifar-10-batches-py/cifar-10-batches-py/'\n",
    "    global loader\n",
    "    loader = dataloader.cifar_dataloader('cifar10',r=noise_ratio,noise_mode=noise_mode,batch_size=batch_size,num_workers=2,\\\n",
    "        root_dir=data_path,noise_file='%s/%.1f_%s.json'%(data_path,noise_ratio,noise_mode))\n",
    "\n",
    "    # batch shape: images, noise_label, index\n",
    "    train_dataloader = loader.run('warmup')\n",
    "\n",
    "    val_dataloader = loader.run('test')\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "\n",
    "train_dataloader, val_dataloader = cifar10_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ad43e-9b8f-40f2-932f-99c5a05d7e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate(model, val_dataloader):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        top1_sum = 0\n",
    "        top5_sum = 0\n",
    "        c = 0\n",
    "        for b in tqdm(val_dataloader):\n",
    "            x = b[0].cuda()\n",
    "            y = b[1].cuda()\n",
    "\n",
    "            y_hat = model(x)\n",
    "\n",
    "            acc1, acc5 = accuracy(y_hat, y, (1,5))\n",
    "            top1_sum += y.shape[0] * acc1\n",
    "            top5_sum += y.shape[0] * acc5\n",
    "            c += y.shape[0]\n",
    "        \n",
    "        top1_acc = top1_sum.item() / c\n",
    "        top5_acc = top5_sum / c\n",
    "        print(f\"validation acc: top-1: {top1_acc}, top-5: {top5_acc}\")\n",
    "        return top1_acc, top5_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5d5394-54ea-454e-8de2-0a60cbc55f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].flatten().float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1b647b-92ce-4b47-bb69-19f3ee9ab031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "snapshot_name = \"simclr_chkpnt/ckpt_cifar10_resnet50.pth\"\n",
    "\n",
    "\n",
    "def fix_state_dict(state_dict):\n",
    "    # create new OrderedDict that does not contain `module.`\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[len('encoder.module.'):] # remove `module.`\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd220b63-8dde-4387-9031-36f651d95fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_encoder(model, state):\n",
    "    for p in model.encoder.parameters():\n",
    "        p.requires_grad = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b1aa29-9bbe-41f7-b947-863937f6d3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet import SupCEResNet\n",
    "net = \"resnet50\"\n",
    "chekpoint = torch.load(snapshot_name)\n",
    "sd = {}\n",
    "for ke in chekpoint['model']:\n",
    "    nk = ke.replace('module.', '')\n",
    "    sd[nk] = chekpoint['model'][ke]\n",
    "model = SupCEResNet(net, num_classes=10)\n",
    "model.load_state_dict(sd, strict=False)\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03452998-2b20-42a0-b635-a82017cd876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook  import tqdm\n",
    "import copy\n",
    "from resnet import resnet50\n",
    "# Stage-2\n",
    "dw_epochs = 50\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "print_step = 100\n",
    "out_dir=\"cifar10/clf_learning\"\n",
    "switch_encoder(model, False)\n",
    "# validate(model, val_dataloader)\n",
    "print(f\"learnable paramters: {sum(torch.numel(p) for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "# optimizer\n",
    "lc_opt = torch.optim.SGD(model.parameters(), lr=3e-3 , momentum=0.9)\n",
    "lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(lc_opt, dw_epochs, 0)\n",
    "lc_opt.zero_grad()\n",
    "\n",
    "\n",
    "lin_loss_log = []\n",
    "lin_epoch_loss = []\n",
    "acc1_hist = []\n",
    "acc5_hist = []\n",
    "\n",
    "def mlp_train():\n",
    "    model.eval()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_val = 0\n",
    "    for epoch in range(1, dw_epochs + 1):\n",
    "        total_loss = 0\n",
    "        # images, noise_label, index\n",
    "        for batch_idx, (x, y, index) in enumerate(tqdm(train_dataloader)):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            \n",
    "            lc_opt.zero_grad()\n",
    "            loss.backward()\n",
    "            lc_opt.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            lin_loss_log.append(loss.item())\n",
    "            if batch_idx % print_step == 0:\n",
    "                tqdm.write('Loss {:.4f}'.format(loss.item()))\n",
    "\n",
    "        current_epoch_loss = total_loss/len(train_dataloader)\n",
    "        lin_epoch_loss.append(current_epoch_loss)\n",
    "        print('Epoch {}: mean loss={:.4f}'.format(epoch, current_epoch_loss))\n",
    "        lr_sched.step()\n",
    "        print(f\"update lr to: {lc_opt.param_groups[0]['lr']}\")\n",
    "        val,val5 = validate(model, val_dataloader)\n",
    "        acc1_hist.append(val)\n",
    "        acc5_hist.append(val5)\n",
    "        if val > best_val:\n",
    "            best_val = val\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        model.train()\n",
    "        print(f\"learnable paramters: {sum(torch.numel(p) for p in model.parameters() if p.requires_grad)}\")\n",
    "        if epoch % 10 == 0:\n",
    "            with open(f\"{out_dir}/classifier_loss_logs.pkl\", \"wb\") as f:\n",
    "                pickle.dump([lin_loss_log, lin_epoch_loss], f)\n",
    "    return best_model_wts, best_val\n",
    "\n",
    "# wts, best_val = mlp_train()\n",
    "# torch.save(wts, f\"{out_dir}/final_model_wts.bin\")\n",
    "model.load_state_dict(torch.load(f\"{out_dir}/final_model_wts.bin\"))\n",
    "validate(model, val_dataloader)\n",
    "# validate(model, loader.run('eval_train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6191f77e-230b-41d2-ab50-dda97125e1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loading the Classifier Loss logs'''\n",
    "with open(f\"{out_dir}/classifier_loss_logs.pkl\",'rb') as f:\n",
    "    loss = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(loss[1])\n",
    "# plt.xscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classifier Loss')\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "''' plotting the Top-1 & Top-5 accuracy'''\n",
    "plt.plot(acc1_hist)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Top-1 Accuracy')\n",
    "plt.figure()\n",
    "hist5 = [a.item() for a in acc5_hist]\n",
    "plt.plot(hist5)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Top-5 Accuracy')\n",
    "plt.figure()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ed38c4-7c50-492a-9c87-7bfc65d29613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "import dataloader_cifar as dataloader\n",
    "\n",
    "def get_train(model, dataloader, estimate_p_right=True, p_right_threshold=0.5, p_clean_threshold=0.5, CBS=True):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    confidence = []\n",
    "    label_preds = []\n",
    "    crit = nn.CrossEntropyLoss(reduction='none')\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets, index) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda() \n",
    "            outputs = model(inputs)\n",
    "            b_losses = crit(outputs, targets)\n",
    "            probs, b_preds = torch.max(torch.softmax(outputs, dim=-1), dim=-1)\n",
    "            confidence.append(probs)\n",
    "            label_preds.append(b_preds)\n",
    "            losses.append(b_losses)\n",
    "\n",
    "    losses = torch.cat(losses, dim=0).cpu()\n",
    "    confidence = torch.cat(confidence, dim=0).cpu()\n",
    "    label_preds = torch.cat(label_preds, dim=0).cpu()\n",
    "    losses = (losses-losses.min())/(losses.max()-losses.min())\n",
    "    \n",
    "    input_loss = losses.reshape(-1, 1)\n",
    "    confidence = confidence.reshape(-1, 1)\n",
    "    # fit a two-component GMM to the loss\n",
    "    gmm1 = GaussianMixture(n_components=2,max_iter=10,tol=1e-2,reg_covar=5e-4)\n",
    "    gmm1.fit(input_loss)\n",
    "    prob = gmm1.predict_proba(input_loss)\n",
    "    p_clean = prob[:,gmm1.means_.argmin()]\n",
    "    clean = (p_clean > p_clean_threshold)\n",
    "    if estimate_p_right:\n",
    "        gmm2 = GaussianMixture(n_components=2,max_iter=10,tol=1e-2,reg_covar=5e-4)\n",
    "        gmm2.fit(confidence)\n",
    "        print(gmm2.means_)\n",
    "        prob = gmm2.predict_proba(confidence)\n",
    "        p_right = prob[:, gmm2.means_.argmax()]\n",
    "        relabel_indicator = p_right > p_right_threshold\n",
    "        return loader.run('train', clean, p_clean, CBS=CBS,relabel_indicator=relabel_indicator, label_preds=label_preds)\n",
    "\n",
    "    return loader.run('train', clean, p_clean, CBS=CBS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e06c33-776c-42fb-9c7e-56519f024645",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('remember to change rampup when finished searching for best lambda_u')\n",
    "def linear_rampup(current, rampup_length=5):\n",
    "    if rampup_length == 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        current = np.clip(current / rampup_length, 0.0, 1.0)\n",
    "        return float(current)\n",
    "\n",
    "class SemiLoss(object):\n",
    "    def __call__(self, outputs_x, targets_x, outputs_u, targets_u, epoch, lambda_u=75, unsupervised_term=True):\n",
    "        probs_u = torch.softmax(outputs_u, dim=1)\n",
    "\n",
    "        Lx = -torch.mean(torch.sum(F.log_softmax(outputs_x, dim=1) * targets_x, dim=1))\n",
    "        if unsupervised_term:\n",
    "            Lu = torch.mean((probs_u - targets_u) ** 2)\n",
    "            return (Lx, Lu, lambda_u * linear_rampup(epoch))\n",
    "        return (Lx,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c87005-4cab-44a5-acfc-14d7a7ae28c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interleave_offsets(batch, nu):\n",
    "    groups = [batch // (nu + 1)] * (nu + 1)\n",
    "    for x in range(batch - sum(groups)):\n",
    "        groups[-x - 1] += 1\n",
    "    offsets = [0]\n",
    "    for g in groups:\n",
    "        offsets.append(offsets[-1] + g)\n",
    "    assert offsets[-1] == batch\n",
    "    return offsets\n",
    "\n",
    "\n",
    "def interleave(xy, batch):\n",
    "    nu = len(xy) - 1\n",
    "    offsets = interleave_offsets(batch, nu)\n",
    "    xy = [[v[offsets[p]:offsets[p + 1]] for p in range(nu + 1)] for v in xy]\n",
    "    for i in range(1, nu + 1):\n",
    "        xy[0][i], xy[i][i] = xy[i][i], xy[0][i]\n",
    "    return [torch.cat(v, dim=0) for v in xy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbeb061-d6d0-4d97-9b15-dc1b23b01b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kronecker_product(t1, t2):\n",
    "    \"\"\"\n",
    "    Computes the Kronecker product between two tensors.\n",
    "    See https://en.wikipedia.org/wiki/Kronecker_product\n",
    "    \"\"\"\n",
    "    t1_height, t1_width = t1.shape\n",
    "    t2_height, t2_width = t2.shape\n",
    "    out_height = t1_height * t2_height\n",
    "    out_width = t1_width * t2_width\n",
    "\n",
    "    tiled_t2 = t2.repeat(t1_height, t1_width)\n",
    "    expanded_t1 = (\n",
    "        t1.unsqueeze(2)\n",
    "          .unsqueeze(3)\n",
    "          .repeat(1, t2_height, t2_width, 1)\n",
    "          .view(out_height, out_width)\n",
    "    )\n",
    "\n",
    "    return expanded_t1 * tiled_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54485131-85f9-49ef-a80a-2abe25de63d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import ReLU\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "Re = ReLU()\n",
    "src = torch.ones(32,10)\n",
    "lab_num = 32# len(L[0])\n",
    "unlab_num = 64 #len(U)\n",
    "one_vec1 = torch.ones(lab_num).unsqueeze(dim=0).cuda().detach()\n",
    "one_vec = torch.ones(unlab_num).unsqueeze(dim=0).cuda().detach()\n",
    "\n",
    "def graph_structured_regularization(model, L, U, lambda_uu, lambda_lu, tau): #U-unlabled class, L-labled, \n",
    "    A_lu = None\n",
    "    A_uu = None\n",
    "    with torch.no_grad():\n",
    "        if lambda_lu != 0:\n",
    "            Z_l = model.encoder(L[0].cuda())\n",
    "            Z_l = normalize(Z_l,dim=1)\n",
    "        Z_u = model.encoder(U.cuda())\n",
    "        Z_u = normalize(Z_u,dim=1)\n",
    "        if lambda_lu != 0:\n",
    "            A_lu = Re(torch.matmul(Z_l, Z_u.t())-tau)\n",
    "        A_uu = Re(torch.matmul(Z_u, Z_u.t())-tau)\n",
    "    y_hat = L[1]\n",
    "    p = model(U.cuda())\n",
    "    p = torch.softmax(p, dim=-1)\n",
    "    # P_mat = kronecker_product(one_vec, p).reshape(unlab_num, unlab_num, 10)\n",
    "    P_mat = p.repeat(1, unlab_num).view(unlab_num, unlab_num, 10)\n",
    "    P_mat1 = P_mat[:,:32,:] # kronecker_product(one_vec1, p).reshape(unlab_num, lab_num, 10)\n",
    "    # Y_mat = kronecker_product(one_vec,y_hat).reshape(lab_num, unlab_num,10)\n",
    "    Y_mat = y_hat.repeat(1, unlab_num).reshape(lab_num, unlab_num,10)\n",
    "    P_mat1 = torch.transpose(P_mat1,0,1)\n",
    "    norm_lu = torch.mean((P_mat1 - Y_mat) **2)\n",
    "    norm_uu = torch.mean((P_mat - torch.transpose(P_mat,0,1) ** 2))\n",
    "    uu_term = (0.5 * lambda_uu*A_uu*norm_uu).sum()\n",
    "    if lambda_lu != 0:\n",
    "        lu_term = (lambda_lu*A_lu*norm_lu).sum() \n",
    "        return lu_term + uu_term\n",
    "    return uu_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a84da4-9c84-4fc8-b3c1-8f4639eb3e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.notebook  import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "\n",
    "def train(labeled_trainloader, unlabeled_trainloader, model, criterion, T=0.5, alpha=0.75, train_iteration=1024, graph_reg=True, unsupervised_term=True, epochs=100, exp_name=None, lambda_u=75, lambda_uu=0.005, lambda_lu=0.01):\n",
    "    model.load_state_dict(torch.load(f\"cifar10/clf_learning/final_model_wts.bin\"))\n",
    "    acc1_hist = []\n",
    "    acc5_hist = []\n",
    "    losses_x_hist = []\n",
    "    losses_u_hist = []\n",
    "    losses_r_hist = []\n",
    "    best_val = 0\n",
    "    switch_encoder(model, state=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"learnable paramters: {sum(torch.numel(p) for p in model.parameters() if p.requires_grad)}\")\n",
    "    for epoch in range(epochs):\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        losses_x = AverageMeter()\n",
    "        losses_u = AverageMeter()\n",
    "        losses_r = AverageMeter()\n",
    "        time_loss_r = AverageMeter()\n",
    "\n",
    "        ws = AverageMeter()\n",
    "        end = time.time()\n",
    "        labeled_train_iter = iter(labeled_trainloader)\n",
    "        unlabeled_train_iter = iter(unlabeled_trainloader)\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx in tqdm(range(train_iteration)):\n",
    "            try:\n",
    "                inputs_x, targets_x = labeled_train_iter.next()\n",
    "            except:\n",
    "                labeled_train_iter = iter(labeled_trainloader)\n",
    "                inputs_x, targets_x = labeled_train_iter.next()\n",
    "\n",
    "            try:\n",
    "                inputs_u, inputs_u2 = unlabeled_train_iter.next()\n",
    "            except:\n",
    "                unlabeled_train_iter = iter(unlabeled_trainloader)\n",
    "                inputs_u, inputs_u2 = unlabeled_train_iter.next()\n",
    "\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            batch_size = inputs_x.size(0)\n",
    "\n",
    "            # Transform label to one-hot\n",
    "            targets_x = torch.zeros(batch_size, 10).scatter_(1, targets_x.view(-1,1).long(), 1)\n",
    "\n",
    "            inputs_x, targets_x = inputs_x.cuda(), targets_x.cuda(non_blocking=True)\n",
    "            inputs_u = inputs_u.cuda()\n",
    "            inputs_u2 = inputs_u2.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # compute guessed labels of unlabel samples\n",
    "                outputs_u = model(inputs_u)\n",
    "                outputs_u2 = model(inputs_u2)\n",
    "                p = (torch.softmax(outputs_u, dim=1) + torch.softmax(outputs_u2, dim=1)) / 2\n",
    "                pt = p ** (1 / T)\n",
    "                targets_u = pt / pt.sum(dim=1, keepdim=True)\n",
    "                targets_u = targets_u.detach()\n",
    "\n",
    "            # mixup\n",
    "            all_inputs = torch.cat([inputs_x, inputs_u, inputs_u2], dim=0)\n",
    "            all_targets = torch.cat([targets_x, targets_u, targets_u], dim=0)\n",
    "\n",
    "            l = np.random.beta(alpha, alpha)\n",
    "\n",
    "            l = max(l, 1-l)\n",
    "\n",
    "            idx = torch.randperm(all_inputs.size(0))\n",
    "\n",
    "            input_a, input_b = all_inputs, all_inputs[idx]\n",
    "            target_a, target_b = all_targets, all_targets[idx]\n",
    "\n",
    "            mixed_input = l * input_a + (1 - l) * input_b\n",
    "            mixed_target = l * target_a + (1 - l) * target_b\n",
    "\n",
    "            # interleave labeled and unlabed samples between batches to get correct batchnorm calculation \n",
    "            mixed_input = list(torch.split(mixed_input, batch_size))\n",
    "            mixed_input = interleave(mixed_input, batch_size)\n",
    "\n",
    "            logits = [model(mixed_input[0])]\n",
    "            for input in mixed_input[1:]:\n",
    "                logits.append(model(input))\n",
    "\n",
    "            # put interleaved samples back\n",
    "            logits = interleave(logits, batch_size)\n",
    "            logits_x = logits[0]\n",
    "            logits_u = torch.cat(logits[1:], dim=0)\n",
    "\n",
    "            crit_res = criterion(logits_x, mixed_target[:batch_size], logits_u, mixed_target[batch_size:], epoch+batch_idx/train_iteration, unsupervised_term=unsupervised_term, lambda_u=lambda_u)\n",
    "            Lx = crit_res[0]\n",
    "            loss = Lx\n",
    "            if unsupervised_term:\n",
    "                Lu = crit_res[1] \n",
    "                w = crit_res[2]\n",
    "                loss = loss + w * Lu\n",
    "\n",
    "            if graph_reg:\n",
    "                R = graph_structured_regularization(model, (inputs_x, targets_x), torch.cat([inputs_u, inputs_u2], dim=0), lambda_uu=lambda_uu, lambda_lu=lambda_lu, tau=0.5)\n",
    "                loss = loss + R\n",
    "\n",
    "\n",
    "            # record loss\n",
    "            losses.update(loss.item(), inputs_x.size(0))\n",
    "            losses_x.update(Lx.item(), inputs_x.size(0))\n",
    "            if unsupervised_term:\n",
    "                losses_u.update(Lu.item(), inputs_x.size(0))\n",
    "                ws.update(w, inputs_x.size(0))\n",
    "            if graph_reg:\n",
    "                losses_r.update(R.item(), inputs_x.size(0))\n",
    "            \n",
    "\n",
    "            # compute gradient and do SGD step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            # plot progress\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Loss: {loss:.4f} | Loss_x: {loss_x:.4f} | Loss_u: {loss_u:.4f} | Loss_R: {loss_r:.4f} | W: {w:.4f}'.format(\n",
    "                            batch=batch_idx + 1,\n",
    "                            size=train_iteration,\n",
    "                            data=data_time.avg,\n",
    "                            bt=batch_time.avg,\n",
    "                            loss=losses.avg,\n",
    "                            loss_x=losses_x.avg,\n",
    "                            loss_u=losses_u.avg,\n",
    "                            loss_r=losses_r.avg,\n",
    "                            w=ws.avg,\n",
    "                            ))\n",
    "                losses_x_hist.append(losses_x.avg)\n",
    "                if unsupervised_term:\n",
    "                    losses_u_hist.append(losses_u.avg)\n",
    "                if graph_reg:\n",
    "                    losses_r_hist.append(losses_r.avg)\n",
    "\n",
    "            if batch_idx % 500 == 0:\n",
    "                val,val5 = validate(model, val_dataloader)\n",
    "                acc1_hist.append(val)\n",
    "                acc5_hist.append(val5)\n",
    "                \n",
    "                if val > best_val:\n",
    "                    best_val = val\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "\n",
    "    torch.save({'losses_x_hist': losses_x_hist, 'losses_u_hist': losses_u_hist, 'losses_r_hist': losses_r_hist, 'best_model_wts': best_model_wts, 'best_val': best_val, 'acc1_hist': acc1_hist, 'acc5_hist': acc5_hist}, f\"runs/{exp_name}.bin\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6bfd62-21cc-4130-8d02-4ac861ac5fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib import pyplot as plt\n",
    "def movingaverage(interval, window_size):\n",
    "    window= np.ones(int(window_size))/float(window_size)\n",
    "    return np.convolve(interval, window, 'same')\n",
    "\n",
    "def plot_result(result):\n",
    "    fig, axs = plt.subplots(2,2, figsize=(25, 10))\n",
    "    def get_avg_x_axs(hist):\n",
    "        if len(hist) == 0:\n",
    "            return [], []\n",
    "        values = map(lambda t:  (t[0]%11 != 0, (((1024 * (t[0]//11)) + (t[0]% 11) * 100)/ 1024), t[1]), enumerate(hist))\n",
    "        filtered_values = filter(lambda t: t[0], values)\n",
    "        return zip(*map(lambda t: (t[1], t[2]), filtered_values))\n",
    "\n",
    "\n",
    "    acc1_x_axs = list(map(lambda x: ((1024 * (x//3)) + [0, 500, 1000][x%3]) / 1024, range(len(result['acc1_hist']))))\n",
    "    x,y = get_avg_x_axs(result['losses_x_hist'])\n",
    "    axs[0,0].plot(x, y)\n",
    "    axs[0,0].set_title('Lx')\n",
    "    axs[0,0].set_xlabel('epoch')\n",
    "    x, y = get_avg_x_axs(result['losses_r_hist'])\n",
    "    axs[0,1].plot(x, y)\n",
    "    axs[0,1].set_title('R')\n",
    "    axs[0,1].set_xlabel('epoch')\n",
    "    x, y = get_avg_x_axs(result['losses_u_hist'])\n",
    "    axs[1,0].plot(x, y)\n",
    "    axs[1,0].set_title('Lu')\n",
    "    axs[1,0].set_xlabel('epoch')\n",
    "    axs[1,1].plot(acc1_x_axs, result['acc1_hist'])\n",
    "    axs[1,1].set_title('Top-1 acc')\n",
    "    axs[1,1].set_xlabel('epoch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aa8461-e508-4168-9d1f-30da62fc6f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs_labeled_trainloader, cbs_unlabeled_trainloader = get_train(model, loader.run('eval_train'), estimate_p_right=True, CBS=True)\n",
    "labeled_trainloader, unlabeled_trainloader = get_train(model, loader.run('eval_train'), estimate_p_right=True, CBS=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f08b59-4391-4590-8f0f-d3dc6c5e75f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBS: True, GMM1: true, GMM2: true, unsupervised_term: false....\n",
    "# train(cbs_labeled_trainloader, cbs_unlabeled_trainloader, model, criterion=SemiLoss(), T=0.5, alpha=0.75, train_iteration=1024, unsupervised_term=True, graph_reg=True, exp_name='reproduce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e1d644-382e-4e10-8bf9-a7fb175af699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBS: True, GMM1: true, GMM2: true, unsupervised_term: false....\n",
    "# train(cbs_labeled_trainloader, cbs_unlabeled_trainloader, model, criterion=SemiLoss(), T=0.5, alpha=0.75, train_iteration=1024, unsupervised_term=False, graph_reg=True, exp_name='no_unsupervised_term')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfc9a5b-1492-41e8-ae72-dfb15780b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_name='no_lambda_lu'\n",
    "# train(cbs_labeled_trainloader, cbs_unlabeled_trainloader, model, criterion=SemiLoss(), T=0.5, alpha=0.75, train_iteration=1024, unsupervised_term=False, graph_reg=True, exp_name=exp_name, lambda_lu=0)\n",
    "# plot_result(torch.load(f'runs/{exp_name}.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f995e6-446a-4087-bd15-9fac97d85c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name='no_lambda_uu'\n",
    "train(cbs_labeled_trainloader, cbs_unlabeled_trainloader, model, criterion=SemiLoss(), T=0.5, alpha=0.75, train_iteration=1024, unsupervised_term=False, graph_reg=True, exp_name=exp_name, lambda_uu=0)\n",
    "plot_result(torch.load(f'runs/{exp_name}.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18772ccb-d832-4af3-b0e4-a9eb36426dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_u_search\n",
    "# for current_lambda_u in [2 , 5 , 10 , 15, 20, 30, 150]:\n",
    "#     model.load_state_dict(torch.load(f\"cifar10/clf_learning/final_model_wts.bin\"))\n",
    "#     train(cbs_labeled_trainloader, cbs_unlabeled_trainloader, model, criterion=SemiLoss(), T=0.5, alpha=0.75, train_iteration=1024, unsupervised_term=True, graph_reg=True, exp_name=f'reproduce_lu_{current_lambda_u}', epochs=7, lambda_u=current_lambda_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0976459a-52aa-4da5-97a6-2ceebbe01097",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
